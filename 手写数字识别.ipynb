{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "手写数字识别.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1I5abQOlnLKj4weNyHJh9P0ugGZbTxcZO",
      "authorship_tag": "ABX9TyO9bx14MR4I+bxiNESYtYxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haonanzhang314/Machine-Learning/blob/master/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVQ328avNOrW",
        "outputId": "0d646ac5-19b3-4d0e-a199-17ec42b2c388"
      },
      "source": [
        "\n",
        "\"\"\"手写数字的识别流程\n",
        "准备数据集包含训练集和测试集\n",
        "将数据集的图片做尺寸归一化，保持相同大小\n",
        "1、加载必要的库：nn网络库、优化器\n",
        "2、定义超参数//例如定义循环的次数。\n",
        "#batch_size批处理。每次处理的数据数量,意思为将海量的数据分批输入电脑中减少压力，一般设置为64,128，根据设备的性能调节\n",
        "#如果有GPU则用GPU训练，否则用cpu训练\n",
        "#EPOCHS:训练数据集的轮次，意思是你可以选择将6万张照片训练一次、10次、100次。。。。\n",
        "3、构建transforms，主要是对图像做变换\n",
        "#构建pipeline也就transfoms,对图像进行处理\n",
        "#可以对图片转换成tensor，旋转图片，以及正则化，明亮度，等等进行处理\n",
        "#将图片转换成tensor格式\n",
        "# nomalize正则化。模型出现过拟合现象时，降低模型复杂度\n",
        "# 过拟合的含义：例如你训练出的模型只认识你自己写的字迹，你朋友写的字迹就不认识了。只认识见过的一模一样的，稍微改变一点就不认识了。\n",
        "transforms.Normalize((0.1307,), (0.3081,)) #正则化，如果不确定取多少值，那就选择这个官网给的值\n",
        "4、下载、加载数据集MNIST\n",
        "#dataloader对数据进行处理\n",
        "#下载数据集\n",
        "#train训练集train是true，test集train就要是false假了，transform=pipline是将数据transfom对数据处理\n",
        "#加载数据\n",
        "#shuffle=True是将图片打乱，是训练图片无顺序，有助于模型精度提高\n",
        "5、构建网络模型------------重要\n",
        "#构建名叫Digit网络模型继承nn.Module\n",
        "#卷积层第1层 1: 灰度图片通道 10：输出通道 5：kernel卷积核\n",
        "#卷积层第2层 10：输入通道 20输出通道 3：kernel卷积核\n",
        "#全连接层1 20*10*10：输入通道 ，500输出通道\n",
        "#全连接层2 500:输入通道 10输出通道\n",
        "# 前向传播\n",
        "# batch_size*1*28*28为1灰度28是像素\n",
        "# batch*1*28*28 ，输出：batch*10*24*24   24是28-5+1=24\n",
        "# 激活函数的作用就是，在所有的隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数，神经网络的表达能力就更加强大了 保持shpae不变，输出：batch*10*24*24\n",
        "# 池化层，将特征更加明显，不明显的忽略。类似于降噪。 输入：batch*10*24*24 输出：batch*10*12*12\n",
        "# 卷积层二层 输入：batch*10*12*12 输出层：batch*20*10*10 （12-3+1=10）\n",
        "# 拉伸，将矩阵形式的图片拉伸成一列 -1自动计算维度，20*10*10=2000\n",
        "# 输入：batch*2000 输出：batch*500\n",
        "# 输入：batch*500 输出：batch*10\n",
        "# 计算分类后，每个数字的概率值\n",
        "#返回概率值\n",
        "6、定义优化器\n",
        "#优化器的作用是更新模型的参数，使得最终的结果达到最优值\n",
        "7、定义训练方法\n",
        "# 模型训练\n",
        "# 把训练部署到DEVICE上\n",
        "# 梯度初始化为0\n",
        "# 训练后的结果\n",
        "# 计算损失\n",
        "# 反向传播\n",
        "# 参数优化\n",
        "# 打印\n",
        "8、定义测试方法\n",
        "# 模型验证\n",
        "# 统计正确率\n",
        "# 测试损失\n",
        "#此处不计算梯度，也不反向传播\n",
        "#部署到device上\n",
        "# 测试数据\n",
        "# 计算测试损失\n",
        "# 找到概率值最大的下标\n",
        "# 累计正确的值\n",
        "9、调用方法 ，开始训练模型.\"\"\"\n",
        "\"\"\"专业名词解释\n",
        "    1、参数与超参数\n",
        "    参数：模型f（x,$）中的$称为模型的参数，可以通过优化算法进行学习。\n",
        "    超参数：用来定义模型结构或优化策略。\n",
        "    2、batch_size批处理。每次处理的数据数量\n",
        "    3、epoch轮次。把一个数据集，循环运行几轮。\n",
        "    4、transforms变换。主要是将图片转换为tensor，旋转图片，以及正则化\n",
        "    5、nomalize正则化。模型出现过拟合现象时，降低模型复杂度。\n",
        "    6、卷积层。由卷积核构建，卷积核简称为卷积，也称为滤波器。卷积的大小可以在实际需要的时候自定义其长和宽（1*1,3*3，5*5）\n",
        "    7、池化层。对图片进行压缩（降采样）的一种方法，如max pooling，average pooling等\n",
        "    8、激活层。激活函数的作用就是，在所有隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数了，因而神经网络的表达能力就更加强大了\n",
        "    9、损失函数。在深度学习中，损失函数反应了模型最后预测效果和实际真值之间的差距，可以用来分析训练过程的好坏、模型是否收敛等、\n",
        "    例如均方损失、交叉熵损失\"\"\"\n",
        "\n",
        "#加载必要的库\n",
        "import torch\n",
        "#nn网络库\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#优化器\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#定义超参数\n",
        "#batch_size批处理。每次处理的数据数量,\n",
        "#意思为将海量的数据分批输入电脑中减少压力，一般设置为64,128，根据设备的性能调节\n",
        "BATCH_SIZE = 64\n",
        "#如果有GPU则用GPU训练，否则用cpu训练\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#训练数据集的轮次，意思是你可以选择将6万张照片训练一次、10次、100次。。。。\n",
        "EPOCHS = 20\n",
        "\n",
        "#构建pipeline也就transfoms,对图像进行处理\n",
        "#可以对图片转换成tensor，旋转图片，以及正则化，明亮度，等等进行处理\n",
        "pipeline = transforms.Compose([\n",
        "    transforms.ToTensor(),#将图片转换成tensor格式\n",
        "    #nomalize正则化。模型出现过拟合现象时，降低模型复杂度\n",
        "    #过拟合的含义：例如你训练出的模型只认识你自己写的字迹，你朋友写的字迹就不认识了。只认识见过的一模一样的，稍微改变一点就不认识了。\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) #正则化，如果不确定取多少值，那就选择这个官网给的值\n",
        "])\n",
        "\n",
        "#下载、加载数据·\n",
        "#dataloader对数据进行处理\n",
        "from torch.utils.data import DataLoader\n",
        "#下载数据集\n",
        "#train训练集train是true，test集train就要是false假了，transform=pipline是将数据transfom对数据处理\n",
        "train_set = datasets.MNIST(\"data\", train=True, download=True, transform=pipeline)\n",
        "test_set =datasets.MNIST(\"data\", train=False, download=True, transform=pipeline)\n",
        "#加载数据\n",
        "#shuffle=True是将图片打乱，是训练图片无顺序，有助于模型精度提高\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#构建网络模型\n",
        "class Digit(nn.Module): #构建名叫Digit网络模型继承nn.Module\n",
        "    def __init__(self): #构造方法\n",
        "        super().__init__() #调用方法\n",
        "        self.conv1 = nn.Conv2d(1, 10, 5)#卷积层第1层 1: 灰度图片通道 10：输出通道 5：kernel卷积核\n",
        "        self.conv2 = nn.Conv2d(10, 20, 3)#卷积层第2层 10：输入通道 20输出通道 3：kernel卷积核\n",
        "        self.fc1 = nn.Linear(20*10*10, 500)#全连接层1 20*10*10：输入通道 ，500输出通道\n",
        "        self.fc2 = nn.Linear(500, 10)#全连接层2 500:输入通道 10输出通道\n",
        "\n",
        "        #前向传播\n",
        "    def forward(self, x):\n",
        "        input_size = x.size(0) #batch_size*1*28*28为1灰度28是像素\n",
        "        x = self.conv1(x) #batch*1*28*28 ，输出：batch*10*24*24   24是28-5+1=24\n",
        "        x = F.relu(x)#激活函数的作用就是，在所有的隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数，神经网络的表达能力就更加强大了 保持shpae不变，输出：batch*10*24*24\n",
        "        x = F.max_pool2d(x, 2, 2) #池化层，将特征更加明显，不明显的忽略。类似于降噪。 输入：batch*10*24*24 输出：batch*10*12*12\n",
        "\n",
        "\n",
        "        x = self.conv2(x)#卷积层二层 输入：batch*10*12*12 输出层：batch*20*10*10 （12-3+1=10）\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = x.view(input_size, -1)#拉伸，将矩阵形式的图片拉伸成一列 -1自动计算维度，20*10*10=2000\n",
        "\n",
        "        x = self.fc1(x) #输入：batch*2000 输出：batch*500\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x) #输入：batch*500 输出：batch*10\n",
        "\n",
        "        output = F.log_softmax(x, dim=1) #计算分类后，每个数字的概率值\n",
        "\n",
        "        return output #返回概率值\n",
        "\n",
        "#定义optimizer优化器 作用更新模型参数，使得最终的训练和测试的结果达到最优值\n",
        "model = Digit().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "#定义训练的函数方法\n",
        "def train_model(model, device, train_loader, optimizer, epoch):\n",
        "    #模型训练\n",
        "    model.train() #调用模型的train方法开始训练\n",
        "    #batch_index每次读取下标， target是标签例如图片是5,则标签是5.\n",
        "    for batch_index, (data, target) in enumerate(train_loader):\n",
        "        #把训练部署到设备上去\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        #梯度初始化为0\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_index % 3000 == 0:\n",
        "            print(\"Train Epoch : {}\\t Loss : {:.6f}\".format(epoch,loss.item()))\n",
        "\n",
        "# 定义测试方法\n",
        "def test_model(model, device, test_loader):\n",
        "    model.eval()\n",
        "    #正确率\n",
        "    correct = 0.0\n",
        "\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target).item()\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        #{:.4f} 表示小数点后4位\n",
        "        print(\"Test --Average loss:{:.4f},Accuracy : {:.3f}\\n\".format(\n",
        "            test_loss, 100.0 * correct / len(test_loader.dataset)))\n",
        "\n",
        "#调用方法7/8\n",
        "for epoch in range(1,EPOCHS +1):\n",
        "    train_model(model,DEVICE,train_loader,optimizer,epoch)\n",
        "    test_model(model,DEVICE,test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch : 1\t Loss : 2.310524\n",
            "Test --Average loss:0.0008,Accuracy : 98.400\n",
            "\n",
            "Train Epoch : 2\t Loss : 0.010149\n",
            "Test --Average loss:0.0005,Accuracy : 98.790\n",
            "\n",
            "Train Epoch : 3\t Loss : 0.001014\n",
            "Test --Average loss:0.0004,Accuracy : 99.040\n",
            "\n",
            "Train Epoch : 4\t Loss : 0.002291\n",
            "Test --Average loss:0.0005,Accuracy : 99.010\n",
            "\n",
            "Train Epoch : 5\t Loss : 0.003368\n",
            "Test --Average loss:0.0005,Accuracy : 99.070\n",
            "\n",
            "Train Epoch : 6\t Loss : 0.003092\n",
            "Test --Average loss:0.0005,Accuracy : 99.070\n",
            "\n",
            "Train Epoch : 7\t Loss : 0.007679\n",
            "Test --Average loss:0.0006,Accuracy : 98.830\n",
            "\n",
            "Train Epoch : 8\t Loss : 0.005191\n",
            "Test --Average loss:0.0005,Accuracy : 99.130\n",
            "\n",
            "Train Epoch : 9\t Loss : 0.012384\n",
            "Test --Average loss:0.0006,Accuracy : 99.090\n",
            "\n",
            "Train Epoch : 10\t Loss : 0.002222\n",
            "Test --Average loss:0.0007,Accuracy : 98.890\n",
            "\n",
            "Train Epoch : 11\t Loss : 0.005800\n",
            "Test --Average loss:0.0007,Accuracy : 99.080\n",
            "\n",
            "Train Epoch : 12\t Loss : 0.000001\n",
            "Test --Average loss:0.0007,Accuracy : 99.060\n",
            "\n",
            "Train Epoch : 13\t Loss : 0.003090\n",
            "Test --Average loss:0.0006,Accuracy : 99.120\n",
            "\n",
            "Train Epoch : 14\t Loss : 0.000017\n",
            "Test --Average loss:0.0006,Accuracy : 99.220\n",
            "\n",
            "Train Epoch : 15\t Loss : 0.000045\n",
            "Test --Average loss:0.0008,Accuracy : 98.930\n",
            "\n",
            "Train Epoch : 16\t Loss : 0.000552\n",
            "Test --Average loss:0.0008,Accuracy : 99.110\n",
            "\n",
            "Train Epoch : 17\t Loss : 0.000003\n",
            "Test --Average loss:0.0007,Accuracy : 99.200\n",
            "\n",
            "Train Epoch : 18\t Loss : 0.000523\n",
            "Test --Average loss:0.0014,Accuracy : 98.660\n",
            "\n",
            "Train Epoch : 19\t Loss : 0.000807\n",
            "Test --Average loss:0.0010,Accuracy : 99.140\n",
            "\n",
            "Train Epoch : 20\t Loss : 0.000013\n",
            "Test --Average loss:0.0009,Accuracy : 98.970\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6FZQ2XgNhnh"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}