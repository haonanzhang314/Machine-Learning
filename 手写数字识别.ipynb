{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "手写数字识别.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1I5abQOlnLKj4weNyHJh9P0ugGZbTxcZO",
      "authorship_tag": "ABX9TyOZ9llshZH2J7ZJ4T09ZkNS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haonanzhang314/Machine-Learning/blob/master/%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVQ328avNOrW",
        "outputId": "48453c89-bdf0-4e04-bcc0-752b7cfc7b0e"
      },
      "source": [
        "\n",
        "\"\"\"手写数字的识别流程\n",
        "准备数据集包含训练集和测试集\n",
        "将数据集的图片做尺寸归一化，保持相同大小\n",
        "1、加载必要的库：nn网络库、优化器\n",
        "2、定义超参数//例如定义循环的次数。\n",
        "#batch_size批处理。每次处理的数据数量,意思为将海量的数据分批输入电脑中减少压力，一般设置为64,128，根据设备的性能调节\n",
        "#如果有GPU则用GPU训练，否则用cpu训练\n",
        "#EPOCHS:训练数据集的轮次，意思是你可以选择将6万张照片训练一次、10次、100次。。。。\n",
        "3、构建transforms，主要是对图像做变换\n",
        "#构建pipeline也就transfoms,对图像进行处理\n",
        "#可以对图片转换成tensor，旋转图片，以及正则化，明亮度，等等进行处理\n",
        "#将图片转换成tensor格式\n",
        "# nomalize正则化。模型出现过拟合现象时，降低模型复杂度\n",
        "# 过拟合的含义：例如你训练出的模型只认识你自己写的字迹，你朋友写的字迹就不认识了。只认识见过的一模一样的，稍微改变一点就不认识了。\n",
        "transforms.Normalize((0.1307,), (0.3081,)) #正则化，如果不确定取多少值，那就选择这个官网给的值\n",
        "4、下载、加载数据集MNIST\n",
        "#dataloader对数据进行处理\n",
        "#下载数据集\n",
        "#train训练集train是true，test集train就要是false假了，transform=pipline是将数据transfom对数据处理\n",
        "#加载数据\n",
        "#shuffle=True是将图片打乱，是训练图片无顺序，有助于模型精度提高\n",
        "5、构建网络模型------------重要\n",
        "#构建名叫Digit网络模型继承nn.Module\n",
        "#卷积层第1层 1: 灰度图片通道 10：输出通道 5：kernel卷积核\n",
        "#卷积层第2层 10：输入通道 20输出通道 3：kernel卷积核\n",
        "#全连接层1 20*10*10：输入通道 ，500输出通道\n",
        "#全连接层2 500:输入通道 10输出通道\n",
        "# 前向传播\n",
        "# batch_size*1*28*28为1灰度28是像素\n",
        "# batch*1*28*28 ，输出：batch*10*24*24   24是28-5+1=24\n",
        "# 激活函数的作用就是，在所有的隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数，神经网络的表达能力就更加强大了 保持shpae不变，输出：batch*10*24*24\n",
        "# 池化层，将特征更加明显，不明显的忽略。类似于降噪。 输入：batch*10*24*24 输出：batch*10*12*12\n",
        "# 卷积层二层 输入：batch*10*12*12 输出层：batch*20*10*10 （12-3+1=10）\n",
        "# 拉伸，将矩阵形式的图片拉伸成一列 -1自动计算维度，20*10*10=2000\n",
        "# 输入：batch*2000 输出：batch*500\n",
        "# 输入：batch*500 输出：batch*10\n",
        "# 计算分类后，每个数字的概率值\n",
        "#返回概率值\n",
        "6、定义优化器\n",
        "#优化器的作用是更新模型的参数，使得最终的结果达到最优值\n",
        "7、定义训练方法\n",
        "# 模型训练\n",
        "# 把训练部署到DEVICE上\n",
        "# 梯度初始化为0\n",
        "# 训练后的结果\n",
        "# 计算损失\n",
        "# 反向传播\n",
        "# 参数优化\n",
        "# 打印\n",
        "8、定义测试方法\n",
        "# 模型验证\n",
        "# 统计正确率\n",
        "# 测试损失\n",
        "#此处不计算梯度，也不反向传播\n",
        "#部署到device上\n",
        "# 测试数据\n",
        "# 计算测试损失\n",
        "# 找到概率值最大的下标\n",
        "# 累计正确的值\n",
        "9、调用方法 ，开始训练模型.\"\"\"\n",
        "\"\"\"专业名词解释\n",
        "    1、参数与超参数\n",
        "    参数：模型f（x,$）中的$称为模型的参数，可以通过优化算法进行学习。\n",
        "    超参数：用来定义模型结构或优化策略。\n",
        "    2、batch_size批处理。每次处理的数据数量\n",
        "    3、epoch轮次。把一个数据集，循环运行几轮。\n",
        "    4、transforms变换。主要是将图片转换为tensor，旋转图片，以及正则化\n",
        "    5、nomalize正则化。模型出现过拟合现象时，降低模型复杂度。\n",
        "    6、卷积层。由卷积核构建，卷积核简称为卷积，也称为滤波器。卷积的大小可以在实际需要的时候自定义其长和宽（1*1,3*3，5*5）\n",
        "    7、池化层。对图片进行压缩（降采样）的一种方法，如max pooling，average pooling等\n",
        "    8、激活层。激活函数的作用就是，在所有隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数了，因而神经网络的表达能力就更加强大了\n",
        "    9、损失函数。在深度学习中，损失函数反应了模型最后预测效果和实际真值之间的差距，可以用来分析训练过程的好坏、模型是否收敛等、\n",
        "    例如均方损失、交叉熵损失\"\"\"\n",
        "\n",
        "#加载必要的库\n",
        "import torch\n",
        "#nn网络库\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#优化器\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "#定义超参数\n",
        "#batch_size批处理。每次处理的数据数量,\n",
        "#意思为将海量的数据分批输入电脑中减少压力，一般设置为64,128，根据设备的性能调节\n",
        "BATCH_SIZE = 64\n",
        "#如果有GPU则用GPU训练，否则用cpu训练\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#训练数据集的轮次，意思是你可以选择将6万张照片训练一次、10次、100次。。。。\n",
        "EPOCHS = 20\n",
        "\n",
        "#构建pipeline也就transfoms,对图像进行处理\n",
        "#可以对图片转换成tensor，旋转图片，以及正则化，明亮度，等等进行处理\n",
        "pipeline = transforms.Compose([\n",
        "    transforms.ToTensor(),#将图片转换成tensor格式\n",
        "    #nomalize正则化。模型出现过拟合现象时，降低模型复杂度\n",
        "    #过拟合的含义：例如你训练出的模型只认识你自己写的字迹，你朋友写的字迹就不认识了。只认识见过的一模一样的，稍微改变一点就不认识了。\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) #正则化，如果不确定取多少值，那就选择这个官网给的值\n",
        "])\n",
        "\n",
        "#下载、加载数据·\n",
        "#dataloader对数据进行处理\n",
        "from torch.utils.data import DataLoader\n",
        "#下载数据集\n",
        "#train训练集train是true，test集train就要是false假了，transform=pipline是将数据transfom对数据处理\n",
        "train_set = datasets.MNIST(\"data\", train=True, download=True, transform=pipeline)\n",
        "test_set =datasets.MNIST(\"data\", train=False, download=True, transform=pipeline)\n",
        "#加载数据\n",
        "#shuffle=True是将图片打乱，是训练图片无顺序，有助于模型精度提高\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#构建网络模型\n",
        "class Digit(nn.Module): #构建名叫Digit网络模型继承nn.Module\n",
        "    def __init__(self): #构造方法\n",
        "        super().__init__() #调用方法\n",
        "        self.conv1 = nn.Conv2d(1, 10, 5)#卷积层第1层 1: 灰度图片通道 10：输出通道 5：kernel卷积核\n",
        "        self.conv2 = nn.Conv2d(10, 20, 3)#卷积层第2层 10：输入通道 20输出通道 3：kernel卷积核\n",
        "        self.fc1 = nn.Linear(20*10*10, 500)#全连接层1 20*10*10：输入通道 ，500输出通道\n",
        "        self.fc2 = nn.Linear(500, 10)#全连接层2 500:输入通道 10输出通道\n",
        "\n",
        "        #前向传播\n",
        "    def forward(self, x):\n",
        "        input_size = x.size(0) #batch_size*1*28*28为1灰度28是像素\n",
        "        x = self.conv1(x) #batch*1*28*28 ，输出：batch*10*24*24   24是28-5+1=24\n",
        "        x = F.relu(x)#激活函数的作用就是，在所有的隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数，神经网络的表达能力就更加强大了 保持shpae不变，输出：batch*10*24*24\n",
        "        x = F.max_pool2d(x, 2, 2) #池化层，将特征更加明显，不明显的忽略。类似于降噪。 输入：batch*10*24*24 输出：batch*10*12*12\n",
        "\n",
        "\n",
        "        x = self.conv2(x)#卷积层二层 输入：batch*10*12*12 输出层：batch*20*10*10 （12-3+1=10）\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = x.view(input_size, -1)#拉伸，将矩阵形式的图片拉伸成一列 -1自动计算维度，20*10*10=2000\n",
        "\n",
        "        x = self.fc1(x) #输入：batch*2000 输出：batch*500\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.fc2(x) #输入：batch*500 输出：batch*10\n",
        "\n",
        "        output = F.log_softmax(x, dim=1) #计算分类后，每个数字的概率值\n",
        "\n",
        "        return output #返回概率值\n",
        "\n",
        "#定义optimizer优化器 作用更新模型参数，使得最终的训练和测试的结果达到最优值\n",
        "model = Digit().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "#定义训练的函数方法\n",
        "def train_model(model, device, train_loader, optimizr, epoch):\n",
        "    #模型训练\n",
        "    model.train() #调用模型的train方法开始训练\n",
        "    #batch_index每次读取下标， target是标签例如图片是5,则标签是5.\n",
        "    for batch_index, (data, target) in enumerate(train_loader):\n",
        "        #把训练部署到设备上去\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        #梯度初始化为0\n",
        "        optimizer.zero_grad()\n",
        "        #训练后的结果\n",
        "        output = model(data)\n",
        "        #计算损失 损失用function中的函数cross_entropy计算，cross_entropy是交叉熵损失，交叉熵损失适用于多分类的任务\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        #找到概率值最大的下标\n",
        "        pred = output.max(1, keepdim=True)\n",
        "        #反向传播\n",
        "        loss.backward()\n",
        "        #参数的更新优化\n",
        "        optimizer.step()\n",
        "        #每隔3000张图片打印一次结果\n",
        "        if batch_index % 3000 == 0:\n",
        "            #format是进行格式化的转换\n",
        "            print(\"Train Epoch : {}\\t Loss : {:.6f}\".format(epoch,loss.item()))\n",
        "# 定义测试方法\n",
        "def test_model(model, device, test_loader):\n",
        "    # 模型验证\n",
        "    model.eval()\n",
        "    #正确率初始化\n",
        "    correct = 0.0\n",
        "    #测试损失初始化\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():#不计算梯度，也不会进行反向传播\n",
        "        for data, target in test_loader:\n",
        "            #部署到device上\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            #测试后的结果\n",
        "            output = model(data)\n",
        "            #累计测试损失 损失用function中的函数cross_entropy计算，cross_entropy是交叉熵损失，交叉熵损失适用于多分类的任务\n",
        "            test_loss += F.cross_entropy(output, target).item()\n",
        "            #找到概率值最大的下标 \n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            #累计正确的值\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        #测试损失率\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        #正确率\n",
        "        correct  /= len(test_loader.dataset)\n",
        "        #{:.4f} 表示小数点后4位\n",
        "        print(\"Test --Average loss:{:.4f},Accuracy : {:.3f}\\n\".format(\n",
        "            test_loss, 100.0 * correct))\n",
        "            # correct 是测试正确的  len（test_loader.dataset）是数据集。相除得到正确率\n",
        "\n",
        "#调用方法7/8\n",
        "for epoch in range(1,EPOCHS +1):\n",
        "    train_model(model,DEVICE,train_loader,optimizer,epoch)\n",
        "    test_model(model,DEVICE,test_loader)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch : 1\t Loss : 2.308326\n",
            "Test --Average loss:0.0007,Accuracy : 98.460\n",
            "\n",
            "Train Epoch : 2\t Loss : 0.013952\n",
            "Test --Average loss:0.0006,Accuracy : 98.680\n",
            "\n",
            "Train Epoch : 3\t Loss : 0.031765\n",
            "Test --Average loss:0.0006,Accuracy : 98.710\n",
            "\n",
            "Train Epoch : 4\t Loss : 0.007598\n",
            "Test --Average loss:0.0005,Accuracy : 99.030\n",
            "\n",
            "Train Epoch : 5\t Loss : 0.001636\n",
            "Test --Average loss:0.0006,Accuracy : 98.780\n",
            "\n",
            "Train Epoch : 6\t Loss : 0.059467\n",
            "Test --Average loss:0.0006,Accuracy : 98.940\n",
            "\n",
            "Train Epoch : 7\t Loss : 0.000427\n",
            "Test --Average loss:0.0006,Accuracy : 99.000\n",
            "\n",
            "Train Epoch : 8\t Loss : 0.003582\n",
            "Test --Average loss:0.0007,Accuracy : 98.930\n",
            "\n",
            "Train Epoch : 9\t Loss : 0.001118\n",
            "Test --Average loss:0.0008,Accuracy : 98.760\n",
            "\n",
            "Train Epoch : 10\t Loss : 0.000021\n",
            "Test --Average loss:0.0008,Accuracy : 99.000\n",
            "\n",
            "Train Epoch : 11\t Loss : 0.000002\n",
            "Test --Average loss:0.0005,Accuracy : 99.250\n",
            "\n",
            "Train Epoch : 12\t Loss : 0.000207\n",
            "Test --Average loss:0.0008,Accuracy : 98.880\n",
            "\n",
            "Train Epoch : 13\t Loss : 0.000074\n",
            "Test --Average loss:0.0005,Accuracy : 99.280\n",
            "\n",
            "Train Epoch : 14\t Loss : 0.000152\n",
            "Test --Average loss:0.0008,Accuracy : 98.830\n",
            "\n",
            "Train Epoch : 15\t Loss : 0.011387\n",
            "Test --Average loss:0.0011,Accuracy : 98.790\n",
            "\n",
            "Train Epoch : 16\t Loss : 0.000020\n",
            "Test --Average loss:0.0007,Accuracy : 99.050\n",
            "\n",
            "Train Epoch : 17\t Loss : 0.004953\n",
            "Test --Average loss:0.0007,Accuracy : 99.050\n",
            "\n",
            "Train Epoch : 18\t Loss : 0.000003\n",
            "Test --Average loss:0.0009,Accuracy : 99.070\n",
            "\n",
            "Train Epoch : 19\t Loss : 0.000002\n",
            "Test --Average loss:0.0011,Accuracy : 98.900\n",
            "\n",
            "Train Epoch : 20\t Loss : 0.000001\n",
            "Test --Average loss:0.0011,Accuracy : 99.050\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMcTBlA4ZFoa",
        "outputId": "15e72841-6b89-4eb7-cfe3-22fe7ea032d4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  4 00:42:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    58W / 149W |    635MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}