{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc687c76",
   "metadata": {},
   "source": [
    "手写数字的识别流程\n",
    "1、准备数据集包含训练集和测试集\n",
    "2、将数据集的图片做尺寸归一化，保持相同大小\n",
    "3、定义超参数//例如定义循环的次数。\n",
    "4、构建transforms，主要是对图像做变换\n",
    "5、下载、加载数据集MNIST\n",
    "6、构建网络模型------------重要\n",
    "7、定义训练方法\n",
    "8、定义测试方法\n",
    "9、开始训练模型，输出预测结果\n",
    "专业名词解释\n",
    "    1、参数与超参数\n",
    "    参数：模型f（x,$）中的$称为模型的参数，可以通过优化算法进行学习。\n",
    "    超参数：用来定义模型结构或优化策略。\n",
    "    2、batch_size批处理。每次处理的数据数量\n",
    "    3、epoch轮次。把一个数据集，循环运行几轮。\n",
    "    4、transforms变换。主要是将图片转换为tensor，旋转图片，以及正则化\n",
    "    5、nomalize正则化。模型出现过拟合现象时，降低模型复杂度。\n",
    "    6、卷积层。由卷积核构建，卷积核简称为卷积，也称为滤波器。卷积的大小可以在实际需要的时候自定义其长和宽（1*1,3*3，5*5）\n",
    "    7、池化层。对图片进行压缩（降采样）的一种方法，如max pooling，average pooling等\n",
    "    8、激活层。激活函数的作用就是，在所有隐藏层之间添加一个激活函数，这样的输出就是一个非线性函数了，因而神经网络的表达能力就更加强大了\n",
    "    9、损失函数。在深度学习中，损失函数反应了模型最后预测效果和实际真值之间的差距，可以用来分析训练过程的好坏、模型是否收敛等、\n",
    "    例如均方损失、交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7578b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载必要的库\n",
    "import torch\n",
    "#nn网络库\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#优化器\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3466c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义超参数\n",
    "#batch_size批处理。每次处理的数据数量,\n",
    "#意思为将海量的数据分批输入电脑中减少压力，一般设置为64,128，根据设备的性能调节\n",
    "BATCH_SIZE = 16 \n",
    "#如果有GPU则用GPU训练，否则用cpu训练\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "#训练数据集的轮次，意思是你可以选择将6万张照片训练一次、10次、100次。。。。\n",
    "EPOCHS = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6b65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建pipeline也就transfoms,对图像进行处理\n",
    "#可以对图片转换成tensor，旋转图片，以及正则化，明亮度，等等进行处理\n",
    "pipeline = transforms.Compose([\n",
    "    transforms.ToTensor(),#将图片转换成tensor格式\n",
    "    #nomalize正则化。模型出现过拟合现象时，降低模型复杂度\n",
    "    #过拟合的含义：例如你训练出的模型只认识你自己写的字迹，你朋友写的字迹就不认识了。只认识见过的一模一样的，稍微改变一点就不认识了。\n",
    "    transforms.Normalize((0.1307,),(0.3081)) #正则化，如果不确定取多少值，那就选择这个官网给的值\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52bd9b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "#下载、加载数据·\n",
    "#dataloader对数据进行处理\n",
    "from torch.utils.data import DataLoader\n",
    "#下载数据集\n",
    "#train训练集train是true，test集train就要是false假了，transform=pipline是将数据transfom对数据处理\n",
    "train_set = datasets.MNIST(\"data\",train=True,download=True,transform=pipeline)\n",
    "test_set =datasets.MNIST(\"data\",train=False,download=True,transform=pipeline)\n",
    "#加载数据\n",
    "#shuffle=True是将图片打乱，是训练图片无顺序，有助于模型精度提高\n",
    "train_loader = DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader =  DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d9913",
   "metadata": {},
   "source": [
    "#此处仅为学习如何打开下载的文件,  显示mnist中的图片\n",
    "with open(\"./data/MNIST/raw/train-images-idx3-ubyte\",\"rb\")as f:\n",
    "    file =f.read()\n",
    "image1 =[int(str(item).encode('ascii'),16)for item in file[16:16+784]]\n",
    "print(image1)\n",
    "import cv2\n",
    "import numpy as np\n",
    "imagel_np =np.array(imagel,dtype=np.uint8).reshape(28,28,1)\n",
    "print(image1_np.shape)\n",
    "cv2.imwrite(\"digit.jpg\",imagel_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04eecac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建网络模型\n",
    "class Digit(nn.Module):\n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self.conv1 = nn.Conv2d(1,10,5)#卷积层第一层 1: 灰度图片通道 10：输出通道 5：kernel卷积核\n",
    "        self.conv2 = nn.Conv2d(10,20,3)#第二层 10：输入通道 20输出通道 3：kernel卷积核\n",
    "        self.fc1 = nn.Linear(20*10*10,500)#20*10*10：输入通道 ，500输出通道  全连接层\n",
    "        self.fc2 = nn.Linear(500,10)#500:输入通道 10输出通道\n",
    "        #前向传播\n",
    "    def forward(self,x):\n",
    "        input_size = x.size(0)#batch_size *1*28*28为1灰度28是像素\n",
    "        x = self.conv1(x)#batch*1*28*28 ，输出：batch*10*24*24   24是28-5+1=24\n",
    "        x = F.relu(x)\n",
    "        x = F.max_poo12d(x,2,2)#池化层\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(input_size,-1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x,dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66245993",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0e5f401bef10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#定义优化器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[0;32m     46\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[0;32m     47\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "#定义优化器\n",
    "model = Digit().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6baacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义训练的函数方法\n",
    "def train_model(model,device,train_loader,optimizer,epoch):\n",
    "    #模型训练\n",
    "    model.train()\n",
    "    for batch_index,(data, target) in enumerate(train_loader):\n",
    "        data,target=data.to(device),target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output,target)\n",
    "        pred = output.max(1,keepdim=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 3000 ==0: \n",
    "            print(\"Train Epoch:{}\\t Loss : {:.6f}\".format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义测试方法\n",
    "def test_model(model,device,test_loader):\n",
    "    model.eval()\n",
    "    correct =0.0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target=data.to(device),target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss =F.cross_entropy(outupt,target).item()\n",
    "            pred = output.max(1,keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(\"Test --Average loss:{:.4f},Accuracy : {:.3f}\\n\".format(\n",
    "            test_loss, 100.0*correct/len(test_loader.dataset)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d240c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用方法7/8\n",
    "for epoch in range(1,EPOCHS +1)：\n",
    "    train_model(model,DEVICE,train_loader,optimizer,epoch)\n",
    "    test_model(model,DEVICE,test_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
